{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border-style: none;\">\n",
    "<tr style=\"border-style: none\">\n",
    "<td style=\"border-style: none; width: 1%; font-size: 16px\">Institut f&uuml;r Theoretische Physik<br /> Universit&auml;t zu K&ouml;ln</td>\n",
    "<td style=\"border-style: none; width: 1%; font-size: 16px\">&nbsp;</td>\n",
    "<td style=\"border-style: none; width: 1%; text-align: right; font-size: 16px\">Prof. Dr. Simon Trebst<br /><b>Peter Br&ouml;cker</b></td>\n",
    "</tr>\n",
    "</table>\n",
    "<hr>\n",
    "<h1 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px;\">Computerphysik</h1>\n",
    "<h1 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px;\">Vorlesung &mdash; Programmiertechniken 12 / Part I\n",
    "</h1>\n",
    "<hr>\n",
    "<h3 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px; margin-bottom: 20px;\">Sommersemester 2016</h3>\n",
    "\n",
    "**Website:** [http://www.thp.uni-koeln.de/trebst/Lectures/2016-CompPhys.shtml](http://www.thp.uni-koeln.de/trebst/Lectures/2016-CompPhys.shtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I &mdash; Neuronales Netz mit \"back propagation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyCall\n",
    "@pyimport matplotlib.cm as cm\n",
    "using PyPlot\n",
    "using LaTeXStrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input erzeugen\n",
    "Der Input sind Arrays, die Zahlen kodieren und für die Optimierung zu einer Matrix verbunden werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero = [1,1,1,1,1,1,0,0,0,1,1,0,0,0,1,1,0,0,0,1,1,1,1,1,1];\n",
    "one = [0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0];\n",
    "\n",
    "input_data = hcat(zero, one)';\n",
    "target_data = [0; 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(10, 5))\n",
    "subplot(121)\n",
    "imshow(transpose(reshape(zero, 5, 5)), interpolation=\"none\", cmap=\"gist_gray\")\n",
    "gca()[:xaxis][:set_visible](false)\n",
    "gca()[:yaxis][:set_visible](false)\n",
    "subplot(122)\n",
    "imshow(transpose(reshape(one, 5, 5)), interpolation=\"none\", cmap=\"gist_gray\")\n",
    "gca()[:xaxis][:set_visible](false)\n",
    "gca()[:yaxis][:set_visible](false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktion\n",
    "\n",
    "Als Aktivierungsfunktion der Neuronen dient ein Sigmoid, beschrieben durch die Gleichung\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(z) = \\dfrac{1}{1 + e^{-z}},\n",
    "\\end{equation}\n",
    "\n",
    "wobei in unserem Fall $z$ das Skalarprodukt $z = \\vec{x} \\cdot \\vec{w}$ von Inputvektoren $\\vec{x}$ und Kantengewichten $\\vec{w}$ ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigmoid(z) = 1. ./ (1. + exp(-z))\n",
    "s_prime(z) = sigmoid(z) .* (1. - sigmoid(z));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(10, 5))\n",
    "xs = linspace(-10, 10, 100)\n",
    "plot(xs, sigmoid(xs), label=L\"$\\sigma(z)$\")\n",
    "plot(xs, s_prime(xs), label=L\"$\\sigma^\\prime(z)$\")\n",
    "legend(loc=\"center right\")\n",
    "ylim([-0.05, 1.05])\n",
    "xlabel(L\"$z$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein neuronales Netz bestehend nur aus der Inputebene und der Outputebene, die verbunden sind über Gewichte $w_{i, j}$, die in einer Matrix $W$ gespeichert sind. Für unseren Fall genügt ein Outputneuron $o$, daher hat die Matrix die Dimension $(25, 1)$. Die Optimierung wird über *backpropagation* unter Verwendung einer **quadratischen Kostenfunktion** $C$ durchgeführt, welche die Abweichung des Outputneurons $o$ vom tatsächlichen Wert $t$ misst:\n",
    "\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{2}(t - o)^2\n",
    "\\end{equation}\n",
    "\n",
    "Ausgehend von den aktuellen Gewichten wird die Ableitung dieser Kostenfunktion in Richtung der Gewichte berechnet:\n",
    "\n",
    "\\begin{equation}\n",
    "\\dfrac{\\partial C}{\\partial w_{ij}} = \\dfrac{\\partial E}{\\partial o} \\cdot\\dfrac{\\partial o}{\\partial z}\\cdot\\dfrac{\\partial z}{\\partial w_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "Der Reihe nach ergeben sich also die Terme\n",
    "- Wie verändert sich der Fehler, wenn sich der Output des Neurons ändert?\n",
    "- Wie ändert sich der Output, wenn sich das Skalarprodukt $z$ ändert?\n",
    "- Wie ändert sich das Skalarprodukt $z = \\vec{x} \\cdot \\vec{w}$, wenn sich das Gewicht ändert?\n",
    "\n",
    "Am Schluss verändern wir die Gewichte genau in die Richtung, die durch den Gradienten vorgegeben wird, gewichtet mit einem Faktor $\\gamma$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{w}_{ij} = w_{i, j} - \\gamma * \\dfrac{\\partial C}{\\partial w_{ij}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Initialisiere Gewichte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input = 25\n",
    "n_output = 1\n",
    "O = 2 * (rand(n_input, n_output) - 0.5);\n",
    "println(\"Startpunkt\\n\", round(sigmoid(input_data * O), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lernen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "gamma = 0.1\n",
    "\n",
    "for i in 1:iterations\n",
    "    # Berechne aktuelles Resultat\n",
    "    result = sigmoid(input_data * O)\n",
    "    # Die Abweichung (entspricht erster partiellen Ableitung)\n",
    "    err = target_data - result\n",
    "    \n",
    "    # Ableitung der Sigmoiden (entspricht zweiter partiellen Ableitung)\n",
    "    derivative = s_prime(result)\n",
    "   \n",
    "    # Update durch Matrixprodukt (entspricht dritter partieller Ableitung und Update)\n",
    "    O += gamma * input_data' * (err .* derivative)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"Resultat\\n\", round(sigmoid(input_data * O), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Einfügen von versteckten Ebenen oder *hidden layers* kann dabei helfen Korrelationen aufzudecken, die sonst verborgen blieben. Dazu wird einfach eine weitere Ebene von Neuronen zwischen Input und Output eingefügt und jedes Neuron aus dieser Ebene mit jeweils jedem Neuron der davor und dahinter liegenden Ebene verbunden. Die Optimierung geschieht ausgehend von der Outputebene und nach genau dem gleichen Prinzip wie zuvor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input = 25\n",
    "n_hidden = 10\n",
    "n_output = 1\n",
    "\n",
    "# Matrix des hidden layers\n",
    "H = 2 * (rand(n_input, n_hidden) - 0.5);\n",
    "\n",
    "# Matrix des output layers\n",
    "O = 2 * (rand(n_hidden, n_output) - 0.5);\n",
    "\n",
    "println(\"Startpunkt\\n\", round(sigmoid(sigmoid(input_data * H) * O), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "gamma = 0.1\n",
    "\n",
    "for i in 1:iterations\n",
    "    # Output der layer\n",
    "    hidden = sigmoid(input_data * H)\n",
    "    result = sigmoid(hidden * O)\n",
    "        \n",
    "    # Die Abweichung (entspricht erster partiellen Ableitung)\n",
    "    result_err = target_data - result\n",
    "    result_delta = result_err .* s_prime(result)\n",
    "    \n",
    "    # Propagiere diesen Fehler durch den hidden layer\n",
    "    hidden_err = result_delta * O'\n",
    "    hidden_delta = hidden_err .* s_prime(hidden)\n",
    "        \n",
    "    # Update durch Matrixprodukt (entspricht dritter partieller Ableitung und Update)\n",
    "    O += gamma * hidden' * result_delta\n",
    "    H += gamma * input_data' * hidden_delta\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"Resultat\\n\", round(sigmoid(sigmoid(input_data * H) * O), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfluss der Lernrate $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "cmap = cm.get_cmap(\"winter\")\n",
    "\n",
    "gammas = [0.0001, 0.001, 0.01, 0.1, 0.25, 1.0, 10.0, 100.0]\n",
    "\n",
    "for (g, gamma) in enumerate(gammas)\n",
    "    n_input = 25\n",
    "    n_hidden = 10\n",
    "    n_output = 1\n",
    "\n",
    "    H = 2 * (rand(n_input, n_hidden) - 0.5);\n",
    "    O = 2 * (rand(n_hidden, n_output) - 0.5);\n",
    "\n",
    "    errors = []\n",
    "    x_values = []\n",
    "    for i in 1:iterations\n",
    "        hidden = sigmoid(input_data * H)\n",
    "        result = sigmoid(hidden * O)\n",
    "        \n",
    "        result_err = target_data - result\n",
    "        \n",
    "        if mod(i, 100) == 0\n",
    "            push!(errors, sum(0.5 * result_err.^2))\n",
    "            push!(x_values, i)\n",
    "        end\n",
    "        result_delta = result_err .* s_prime(result)\n",
    "\n",
    "        hidden_err = result_delta * O'\n",
    "        hidden_delta = hidden_err .* s_prime(hidden)\n",
    "\n",
    "        O += gamma * hidden' * result_delta\n",
    "        H += gamma * input_data' * hidden_delta\n",
    "    end\n",
    "    \n",
    "    plot(x_values, errors, linewidth=2, label=L\"$\\gamma=\"*string(gamma)*L\"$\", color=pyeval(\"cmap(g)\", cmap=cmap, g=g/length(gammas)))\n",
    "end\n",
    "gca()[:set_yscale](\"log\")\n",
    "legend(loc=\"lower left\")\n",
    "xlabel(\"Iteration\")\n",
    "ylabel(L\"Error $E$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
