{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border-style: none;\">\n",
    "<tr style=\"border-style: none\">\n",
    "<td style=\"border-style: none; width: 1%; font-size: 16px\">Institut f&uuml;r Theoretische Physik<br /> Universit&auml;t zu K&ouml;ln</td>\n",
    "<td style=\"border-style: none; width: 1%; font-size: 16px\">&nbsp;</td>\n",
    "<td style=\"border-style: none; width: 1%; text-align: right; font-size: 16px\">Prof. Dr. Simon Trebst<br /><b>Peter Br&ouml;cker</b></td>\n",
    "</tr>\n",
    "</table>\n",
    "<hr>\n",
    "<h1 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px;\">Computerphysik</h1>\n",
    "<h1 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px;\">Vorlesung &mdash; Programmiertechniken 12 / Part I\n",
    "</h1>\n",
    "<hr>\n",
    "<h3 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px; margin-bottom: 20px;\">Sommersemester 2016</h3>\n",
    "\n",
    "**Website:** [http://www.thp.uni-koeln.de/trebst/Lectures/2016-CompPhys.shtml](http://www.thp.uni-koeln.de/trebst/Lectures/2016-CompPhys.shtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I &mdash; Neuronales Netz mit \"back propagation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Recompiling stale cache file /home/juser/.julia/lib/v0.4/PyCall.ji for module PyCall.\n",
      "INFO: Recompiling stale cache file /opt/julia_packages/.julia/lib/v0.4/Conda.ji for module Conda.\n",
      "INFO: Recompiling stale cache file /opt/julia_packages/.julia/lib/v0.4/BinDeps.ji for module BinDeps.\n",
      "INFO: Recompiling stale cache file /opt/julia_packages/.julia/lib/v0.4/URIParser.ji for module URIParser.\n",
      "WARNING: Module Compat uuid did not match cache file\n",
      "  This is likely because module Compat does not support  precompilation but is imported by a module that does.\n",
      "WARNING: deserialization checks failed while attempting to load cache from /opt/julia_packages/.julia/lib/v0.4/URIParser.ji\n",
      "INFO: Recompiling stale cache file /opt/julia_packages/.julia/lib/v0.4/SHA.ji for module SHA.\n",
      "WARNING: Module Compat uuid did not match cache file\n",
      "  This is likely because module Compat does not support  precompilation but is imported by a module that does.\n",
      "WARNING: deserialization checks failed while attempting to load cache from /opt/julia_packages/.julia/lib/v0.4/SHA.ji\n",
      "WARNING: Module Compat uuid did not match cache file\n",
      "  This is likely because module Compat does not support  precompilation but is imported by a module that does.\n",
      "WARNING: deserialization checks failed while attempting to load cache from /opt/julia_packages/.julia/lib/v0.4/BinDeps.ji\n",
      "WARNING: Module Compat uuid did not match cache file\n",
      "  This is likely because module Compat does not support  precompilation but is imported by a module that does.\n",
      "WARNING: deserialization checks failed while attempting to load cache from /opt/julia_packages/.julia/lib/v0.4/Conda.ji\n",
      "INFO: Recompiling stale cache file /home/juser/.julia/lib/v0.4/MacroTools.ji for module MacroTools.\n",
      "INFO: Recompiling stale cache file /home/juser/.julia/lib/v0.4/PyPlot.ji for module PyPlot.\n",
      "INFO: Recompiling stale cache file /home/juser/.julia/lib/v0.4/Colors.ji for module Colors.\n",
      "INFO: Recompiling stale cache file /home/juser/.julia/lib/v0.4/FixedPointNumbers.ji for module FixedPointNumbers.\n",
      "INFO: Recompiling stale cache file /home/juser/.julia/lib/v0.4/ColorTypes.ji for module ColorTypes.\n",
      "INFO: Recompiling stale cache file /home/juser/.julia/lib/v0.4/Reexport.ji for module Reexport.\n",
      "INFO: Recompiling stale cache file /home/juser/.julia/lib/v0.4/LaTeXStrings.ji for module LaTeXStrings.\n"
     ]
    }
   ],
   "source": [
    "using PyCall\n",
    "@pyimport matplotlib.cm as cm\n",
    "using PyPlot\n",
    "using LaTeXStrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input erzeugen\n",
    "Der Input sind Arrays, die Zahlen kodieren und f√ºr die Optimierung zu einer Matrix verbunden werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 0\n",
       " 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero = [1,1,1,1,1,1,0,0,0,1,1,0,0,0,1,1,0,0,0,1,1,1,1,1,1];\n",
    "one = [0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0];\n",
    "\n",
    "input_data = hcat(zero, one)';\n",
    "target_data = [0; 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "PyPlot.Figure(PyObject <matplotlib.figure.Figure object at 0x7f8e79a96e50>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(figsize=(10, 5))\n",
    "subplot(121)\n",
    "imshow(transpose(reshape(zero, 5, 5)), interpolation=\"none\", cmap=\"gist_gray\")\n",
    "gca()[:xaxis][:set_visible](false)\n",
    "gca()[:yaxis][:set_visible](false)\n",
    "subplot(122)\n",
    "imshow(transpose(reshape(one, 5, 5)), interpolation=\"none\", cmap=\"gist_gray\")\n",
    "gca()[:xaxis][:set_visible](false)\n",
    "gca()[:yaxis][:set_visible](false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktion\n",
    "\n",
    "Als Aktivierungsfunktion der Neuronen dient ein Sigmoid, beschrieben durch die Gleichung\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(z) = \\dfrac{1}{1 + e^{-z}},\n",
    "\\end{equation}\n",
    "\n",
    "wobei in unserem Fall $z$ das Skalarprodukt $z = \\vec{x} \\cdot \\vec{w}$ von Inputvektoren $\\vec{x}$ und Kantengewichten $\\vec{w}$ ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigmoid(z) = 1. ./ (1. + exp(-z))\n",
    "s_prime(z) = sigmoid(z) .* (1. - sigmoid(z));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(10, 5))\n",
    "xs = linspace(-10, 10, 100)\n",
    "plot(xs, sigmoid(xs), label=L\"$\\sigma(z)$\")\n",
    "plot(xs, s_prime(xs), label=L\"$\\sigma^\\prime(z)$\")\n",
    "legend(loc=\"center right\")\n",
    "ylim([-0.05, 1.05])\n",
    "xlabel(L\"$z$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein neuronales Netz bestehend nur aus der Inputebene und der Outputebene, die verbunden sind √ºber Gewichte $w_{i, j}$, die in einer Matrix $W$ gespeichert sind. F√ºr unseren Fall gen√ºgt ein Outputneuron $o$, daher hat die Matrix die Dimension $(25, 1)$. Die Optimierung wird √ºber *backpropagation* unter Verwendung einer **quadratischen Kostenfunktion** $C$ durchgef√ºhrt, welche die Abweichung des Outputneurons $o$ vom tats√§chlichen Wert $t$ misst:\n",
    "\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{2}(t - o)^2\n",
    "\\end{equation}\n",
    "\n",
    "Ausgehend von den aktuellen Gewichten wird die Ableitung dieser Kostenfunktion in Richtung der Gewichte berechnet:\n",
    "\n",
    "\\begin{equation}\n",
    "\\dfrac{\\partial C}{\\partial w_{ij}} = \\dfrac{\\partial E}{\\partial o} \\cdot\\dfrac{\\partial o}{\\partial z}\\cdot\\dfrac{\\partial z}{\\partial w_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "Der Reihe nach ergeben sich also die Terme\n",
    "- Wie ver√§ndert sich der Fehler, wenn sich der Output des Neurons √§ndert?\n",
    "- Wie √§ndert sich der Output, wenn sich das Skalarprodukt $z$ √§ndert?\n",
    "- Wie √§ndert sich das Skalarprodukt $z = \\vec{x} \\cdot \\vec{w}$, wenn sich das Gewicht √§ndert?\n",
    "\n",
    "Am Schluss ver√§ndern wir die Gewichte genau in die Richtung, die durch den Gradienten vorgegeben wird, gewichtet mit einem Faktor $\\gamma$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{w}_{ij} = w_{i, j} - \\gamma * \\dfrac{\\partial C}{\\partial w_{ij}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Initialisiere Gewichte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input = 25\n",
    "n_output = 1\n",
    "O = 2 * (rand(n_input, n_output) - 0.5);\n",
    "println(\"Startpunkt\\n\", round(sigmoid(input_data * O), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lernen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "gamma = 0.1\n",
    "\n",
    "for i in 1:iterations\n",
    "    # Berechne aktuelles Resultat\n",
    "    result = sigmoid(input_data * O)\n",
    "    # Die Abweichung (entspricht erster partiellen Ableitung)\n",
    "    err = target_data - result\n",
    "    \n",
    "    # Ableitung der Sigmoiden (entspricht zweiter partiellen Ableitung)\n",
    "    derivative = s_prime(result)\n",
    "   \n",
    "    # Update durch Matrixprodukt (entspricht dritter partieller Ableitung und Update)\n",
    "    O += gamma * input_data' * (err .* derivative)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"Resultat\\n\", round(sigmoid(input_data * O), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Einf√ºgen von versteckten Ebenen oder *hidden layers* kann dabei helfen Korrelationen aufzudecken, die sonst verborgen blieben. Dazu wird einfach eine weitere Ebene von Neuronen zwischen Input und Output eingef√ºgt und jedes Neuron aus dieser Ebene mit jeweils jedem Neuron der davor und dahinter liegenden Ebene verbunden. Die Optimierung geschieht ausgehend von der Outputebene und nach genau dem gleichen Prinzip wie zuvor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input = 25\n",
    "n_hidden = 10\n",
    "n_output = 1\n",
    "\n",
    "# Matrix des hidden layers\n",
    "H = 2 * (rand(n_input, n_hidden) - 0.5);\n",
    "\n",
    "# Matrix des output layers\n",
    "O = 2 * (rand(n_hidden, n_output) - 0.5);\n",
    "\n",
    "println(\"Startpunkt\\n\", round(sigmoid(sigmoid(input_data * H) * O), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "gamma = 0.1\n",
    "\n",
    "for i in 1:iterations\n",
    "    # Output der layer\n",
    "    hidden = sigmoid(input_data * H)\n",
    "    result = sigmoid(hidden * O)\n",
    "        \n",
    "    # Die Abweichung (entspricht erster partiellen Ableitung)\n",
    "    result_err = target_data - result\n",
    "    result_delta = result_err .* s_prime(result)\n",
    "    \n",
    "    # Propagiere diesen Fehler durch den hidden layer\n",
    "    hidden_err = result_delta * O'\n",
    "    hidden_delta = hidden_err .* s_prime(hidden)\n",
    "        \n",
    "    # Update durch Matrixprodukt (entspricht dritter partieller Ableitung und Update)\n",
    "    O += gamma * hidden' * result_delta\n",
    "    H += gamma * input_data' * hidden_delta\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"Resultat\\n\", round(sigmoid(sigmoid(input_data * H) * O), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfluss der Lernrate $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "cmap = cm.get_cmap(\"winter\")\n",
    "\n",
    "gammas = [0.0001, 0.001, 0.01, 0.1, 0.25, 1.0, 10.0, 100.0]\n",
    "\n",
    "for (g, gamma) in enumerate(gammas)\n",
    "    n_input = 25\n",
    "    n_hidden = 10\n",
    "    n_output = 1\n",
    "\n",
    "    H = 2 * (rand(n_input, n_hidden) - 0.5);\n",
    "    O = 2 * (rand(n_hidden, n_output) - 0.5);\n",
    "\n",
    "    errors = []\n",
    "    x_values = []\n",
    "    for i in 1:iterations\n",
    "        hidden = sigmoid(input_data * H)\n",
    "        result = sigmoid(hidden * O)\n",
    "        \n",
    "        result_err = target_data - result\n",
    "        \n",
    "        if mod(i, 100) == 0\n",
    "            push!(errors, sum(0.5 * result_err.^2))\n",
    "            push!(x_values, i)\n",
    "        end\n",
    "        result_delta = result_err .* s_prime(result)\n",
    "\n",
    "        hidden_err = result_delta * O'\n",
    "        hidden_delta = hidden_err .* s_prime(hidden)\n",
    "\n",
    "        O += gamma * hidden' * result_delta\n",
    "        H += gamma * input_data' * hidden_delta\n",
    "    end\n",
    "    \n",
    "    plot(x_values, errors, linewidth=2, label=L\"$\\gamma=\"*string(gamma)*L\"$\", color=pyeval(\"cmap(g)\", cmap=cmap, g=g/length(gammas)))\n",
    "end\n",
    "gca()[:set_yscale](\"log\")\n",
    "legend(loc=\"lower left\")\n",
    "xlabel(\"Iteration\")\n",
    "ylabel(L\"Error $E$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
