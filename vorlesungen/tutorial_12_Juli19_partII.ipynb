{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border-style: none;\">\n",
    "<tr style=\"border-style: none\">\n",
    "<td style=\"border-style: none; width: 1%; font-size: 16px\">Institut f&uuml;r Theoretische Physik<br /> Universit&auml;t zu K&ouml;ln</td>\n",
    "<td style=\"border-style: none; width: 1%; font-size: 16px\">&nbsp;</td>\n",
    "<td style=\"border-style: none; width: 1%; text-align: right; font-size: 16px\">Prof. Dr. Simon Trebst<br /><b>Peter Br&ouml;cker</b></td>\n",
    "</tr>\n",
    "</table>\n",
    "<hr>\n",
    "<h1 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px;\">Computerphysik</h1>\n",
    "<h1 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px;\">Vorlesung &mdash; Programmiertechniken 12 / Part II\n",
    "</h1>\n",
    "<hr>\n",
    "<h3 style=\"font-weight:bold; text-align: center; margin: 0px; padding:0px; margin-bottom: 20px;\">Sommersemester 2016</h3>\n",
    "\n",
    "**Website:** [http://www.thp.uni-koeln.de/trebst/Lectures/2016-CompPhys.shtml](http://www.thp.uni-koeln.de/trebst/Lectures/2016-CompPhys.shtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II &mdash; Neuronales Netz mit Stochastic Gradient Descent und Mini Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir arbeiten nun mit dem sogenannten <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST Datensatz</a> handschriftlicher Ziffern, der standardmäßig dazu genutzt wird, um die Qualität verschiedener Designs von neuronalen Netzen zu überprüfen. Dazu laden wir ihn zunächst hinunter und untersuchen dann den Inhalt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyCall\n",
    "@pyimport matplotlib.cm as cm\n",
    "using PyPlot\n",
    "using LaTeXStrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">MNIST Datensatz</a> handschriftlicher Ziffern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if !isfile(\"mnist_train.csv\")\n",
    "    download(\"http://www.thp.uni-koeln.de/trebst/Lectures/CompPhys-2016/mnist_train.csv\")\n",
    "end\n",
    "\n",
    "if !isfile(\"mnist_test.csv\")\n",
    "    download(\"http://www.thp.uni-koeln.de/trebst/Lectures/CompPhys-2016/mnist_test.csv\")\n",
    "end\n",
    "\n",
    "using DataFrames\n",
    "\n",
    "test = readtable(\"mnist_test.csv\", header=false);\n",
    "train = readtable(\"mnist_train.csv\", header=false);        \n",
    "\n",
    "println(\"Größe der Datensätze\")\n",
    "println(size(test))\n",
    "println(size(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die erste Spalte ist das Label, während der Rest eine digitalisierte, handschriftliche Ziffer der Größe $28 \\times 28$ darstellt. Wir separieren nun Label von Zahl und betrachten dann ein paar Beispielziffern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sze = 28\n",
    "train_labels = Array(train[1]);\n",
    "train_labels = zeros(size(train)[1], 10)\n",
    "for i in 1:size(train)[1]\n",
    "    train_labels[i, train[i, 1] + 1] = 1\n",
    "end\n",
    "train_inputs = Array(train[2:end]) / 255;\n",
    "test_labels = zeros(size(test)[1], 10)\n",
    "for i in 1:size(test)[1]\n",
    "    test_labels[i, test[i, 1] + 1] = 1\n",
    "end\n",
    "test_inputs = Array(test[2:end]) / 255;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(10, 10))\n",
    "\n",
    "println(\"Beispiele Training-Set\")\n",
    "subplot(221)\n",
    "println(train_labels[1, :],\"   Ziffer = \", indmax(train_labels[1, :])-1)\n",
    "imshow(transpose(reshape(train_inputs[1, :, :], sze, sze)), interpolation=\"none\", cmap=\"gist_gray\")\n",
    "gca()[:xaxis][:set_visible](false)\n",
    "gca()[:yaxis][:set_visible](false)\n",
    "\n",
    "subplot(222)\n",
    "println(train_labels[2, :],\"   Ziffer = \", indmax(train_labels[2, :])-1)\n",
    "imshow(transpose(reshape(train_inputs[2, :, :], sze, sze)), interpolation=\"none\", cmap=\"gist_gray\")\n",
    "gca()[:xaxis][:set_visible](false)\n",
    "gca()[:yaxis][:set_visible](false)\n",
    "\n",
    "println(\"\\nBeispiele Test-Set\")\n",
    "subplot(223)\n",
    "println(test_labels[5, :],\"   Ziffer = \", indmax(test_labels[5, :])-1)\n",
    "imshow(transpose(reshape(test_inputs[5, :, :], sze, sze)), interpolation=\"none\", cmap=\"gist_gray\")\n",
    "gca()[:xaxis][:set_visible](false)\n",
    "gca()[:yaxis][:set_visible](false)\n",
    "\n",
    "subplot(224)\n",
    "println(test_labels[6, :],\"   Ziffer = \", indmax(test_labels[6, :])-1)\n",
    "imshow(transpose(reshape(test_inputs[6, :, :], sze, sze)), interpolation=\"none\", cmap=\"gist_gray\")\n",
    "gca()[:xaxis][:set_visible](false)\n",
    "gca()[:yaxis][:set_visible](false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anstatt nun alle Datensätze auf einmal zu verwenden, wie wir es in dem ersten, einfacheren Beispiel getan haben, verwenden wir nun einzelne Teilmengen, sogenannte **mini batches**, wobei es wichtig ist, dass diese Teilmengen groß genug sind, um die Gesamtemenge gut genug zu repräsentieren. Es ist wichtig vor jedem Durchlauf die Daten zu mischen, um zu verhindern, dass in den Daten Zyklen enstehen, in denen das Netzwerk hängen bleibt. Die Anzahl der Durchläufe, die mit den gesamten Daten passieren, wird in diesem Kontext als *epoch* bezeichnet. Wie viele man davon braucht, um eine Netzwerk ausreichend gut zu trainineren, hängt sehr stark vom Problem ab und muss empirisch bestimmt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigmoid(x) = 1. ./ (1. + exp(-x))\n",
    "s_prime(x) = sigmoid(x) .* (1. - sigmoid(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Netzwerk-Architektur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_input = 28 * 28\n",
    "n_hidden = 150\n",
    "n_output = 10\n",
    "\n",
    "H = 2 * (rand(n_input, n_hidden) - 0.5);\n",
    "HB = 2 * (rand(1, n_hidden) - 0.5);\n",
    "O = 2 * (rand(n_hidden, n_output) - 0.5);\n",
    "HO = 2 * (rand(1, n_output) - 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(size(train_inputs[1, :] * H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Überwachtes Lernen** des neuronalen Netzes mit **Back Propagation**, **Stochastic Gradient Descent** und **Mini Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "gamma = 0.01 \n",
    "batch_size = 250\n",
    "println(size(train_inputs))\n",
    "println(Int(size(train_inputs)[1] / batch_size))\n",
    "batches = 2 #Int(size(train_inputs)[1] / batch_size)\n",
    "order = collect(1:size(train_inputs)[1])\n",
    "\n",
    "hidden = zeros(n_hidden)\n",
    "result = zeros(n_output)\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    if mod(epoch, 10) == 0\n",
    "        print(\"Epoch:\\t\", epoch, \" - \")\n",
    "    end\n",
    "    shuffle!(order)\n",
    "    for b in 1:batches-1\n",
    "        dO = zeros(O)\n",
    "        dHO = zeros(HO)\n",
    "        dH = zeros(H)\n",
    "        dHB = zeros(HB)\n",
    "        for i in order[b * batch_size:(b + 1) * batch_size - 1]\n",
    "            input_data = train_inputs[i, :]\n",
    "            target_data = train_labels[i, :]\n",
    "\n",
    "            hidden = sigmoid(input_data * H + HB)\n",
    "            result = sigmoid(hidden * O + HO)\n",
    "        \n",
    "            result_err = target_data - result\n",
    "            result_delta = result_err .* s_prime(result)\n",
    "\n",
    "            hidden_err = result_delta * O'\n",
    "            hidden_delta = hidden_err .* s_prime(hidden)\n",
    "\n",
    "            dO += gamma * hidden' * result_delta\n",
    "            dHO += gamma * result_delta\n",
    "            dH += gamma * input_data' * hidden_delta\n",
    "            dHB += gamma * hidden_delta\n",
    "        end\n",
    "        O += dO\n",
    "        HO += dHO\n",
    "        H += dH\n",
    "        HB += dHB\n",
    "    end\n",
    "    \n",
    "    test_error = 0\n",
    "\n",
    "    for i in size(test_inputs)[1]\n",
    "        test_error += sum(0.5*(sigmoid(sigmoid(test_inputs[i, :] * H + HB) * O + HO) - test_labels[i, :]).^2)\n",
    "    end\n",
    "    \n",
    "    if mod(epoch, 10) == 0\n",
    "        println(\"Error:\\t\", test_error)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anwendung** des optimierten Netzes auf den Test-Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check = floor(10000*rand())\n",
    "\n",
    "imshow(transpose(reshape(test_inputs[check, :, :], sze, sze)), interpolation=\"none\", cmap=\"gist_gray\")\n",
    "gca()[:xaxis][:set_visible](false)\n",
    "gca()[:yaxis][:set_visible](false)\n",
    "\n",
    "test_input = test_labels[check, :]\n",
    "println(\"Test-Ziffer = \", indmax(test_input)-1)\n",
    "\n",
    "println(\"\\nAusgabe des neuronalen Netzes\")\n",
    "neuro_result = round(sigmoid(sigmoid(test_inputs[check, :] * H + HB) * O + HO), 3)\n",
    "println(\"   \", neuro_result)\n",
    "println(\"   Erkannte Ziffer = \", indmax(neuro_result)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Berechnung der **Treffer-/Fehlerquote**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oops = 0\n",
    "\n",
    "for i in 1:10000\n",
    "#     check = floor(10000*rand())\n",
    "    check = i\n",
    "\n",
    "    test_input = test_labels[check, :]\n",
    "    neuro_result = round(sigmoid(sigmoid(test_inputs[check, :] * H + HB) * O + HO), 3)\n",
    "\n",
    "    input_digit = indmax(test_input)-1\n",
    "    output_digit = indmax(neuro_result)-1\n",
    "    \n",
    "    if input_digit != output_digit\n",
    "        oops += 1\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Trefferquote ist \", 100-oops/100, \" Prozent.\")\n",
    "println(\"Fehlerquote liegt bei \", oops/100,\" Prozent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
